{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure that Python starts in Workflow-folder or else the modules will be screewed up!\n",
    "import sys, os, getpass\n",
    "from datetime import datetime\n",
    "from py4j import protocol\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print(module_path)\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "if user == \"sidsel\":\n",
    "    parquet_path = \"/home/sidsel/workspace/sparkdata/parquet\"\n",
    "elif user == \"svanhmic\":\n",
    "    parquet_path = \"/home/svanhmic/workspace/data/DABAI/sparkdata/parquet\"\n",
    "    \n",
    "# Start the logger.\n",
    "import logging\n",
    "logger_tester = logging.getLogger(__name__)\n",
    "logger_tester.setLevel(logging.INFO)\n",
    "logger_file_handler_param = logging.FileHandler('/tmp/'+datetime.now().strftime('workflow_test_%d_%m_%Y.log'))\n",
    "logger_formatter_param = logging.Formatter('%(asctime)s:%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "logger_tester.addHandler(logger_file_handler_param)\n",
    "logger_file_handler_param.setFormatter(logger_formatter_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shared.Extension_to_timeit import pretty_time_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(params.output_parameters(parameters))\n",
    "test_params_1 = {'tol': 0.001, 'k': 8, 'maxIter': 10, 'algorithm': 'GaussianMixture', 'seed': 1080866016001745000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sc.defaultMinPartitions)\n",
    "print(sc.defaultParallelism)\n",
    "conf = sc.getConf()\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cleaning.ExecuteCleaningWorkflow import ExecuteWorkflow\n",
    "\n",
    "n = [3, 5, 6] # number of samples 1000, 100000, 1000000\n",
    "\n",
    "execution_model = ExecuteWorkflow(dict_params=test_params_1\n",
    "                                ,cols_features=['a','b']\n",
    "                                ,cols_labels=['id','k','dimension'])\n",
    "\n",
    "n_samples = [1000, 10000]#, 1000000]\n",
    "n_partitions = [40,80,]#200,400,600, 800, 1000]\n",
    "collection_of_data = [parquet_path+'/normal_cluster_n_'+str(i)+'.parquet' for i in n_samples]\n",
    "collection_of_model = []\n",
    "#counts = [i.rdd.getNumPartitions() for i in collection_of_data]\n",
    "#counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for jdx, partition in enumerate(n_partitions):\n",
    "    try:\n",
    "        for idx, data_path in enumerate(collection_of_data):\n",
    "\n",
    "            # Strings \n",
    "            str_1 = 'Iteration: {} - Number of partions: {}'\n",
    "            str_2 = 'Iteration: {} - Training model time: {!s}'\n",
    "            str_3 = 'Iteration: {} - Transforming model time {!s}'\n",
    "            \n",
    "            df_data = (spark.\n",
    "                       read.\n",
    "                       parquet(data_path).\n",
    "                       repartition(partition)\n",
    "                       )\n",
    "\n",
    "            iteration = idx+len(collection_of_data)*jdx\n",
    "            logger_tester.info(\n",
    "                str_1.format(iteration, df_data.rdd.getNumPartitions()))\n",
    "            \n",
    "            model_timer = %timeit -r7 -o collection_of_model.append(execution_model.execute_pipeline(df_data))\n",
    "            transformer_timer = %timeit -o execution_model.apply_model(collection_of_model[iteration],df_data)\n",
    "            collection_of_model = collection_of_model[:iteration+1]\n",
    "            \n",
    "            logger_tester.info(\n",
    "                str_2.format(iteration,pretty_time_result(model_timer)))\n",
    "            logger_tester.info(\n",
    "                str_3.format(iteration,pretty_time_result(transformer_timer)))\n",
    "    except protocol.Py4JError as error:\n",
    "        ex_type, ex, tb = sys.exc_info()\n",
    "        logger_tester.warning('Failed with traceback'+ str(error.with_traceback(tb)))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in samples:\n",
    "    means = create_dummy_data.create_means(dim, k, 10)  # [[0, 0, 0], [3, 3, 3], [-3, 3, -3], [5, -5, 5]]\n",
    "    stds = create_dummy_data.create_stds(dim, k)  # [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]\n",
    "    n_samples = create_dummy_data.create_partition_samples(i, k)  # [1000, 10000, 4000, 50]\n",
    "    print(n_samples)\n",
    "    df = create_dummy_data.create_normal_cluster_data_spark(dim, n_samples, means, stds)\n",
    "    #df.show(100)\n",
    "    df.write.parquet('/user/micsas/data/parquet/normal_cluster_n_'+str(i)+'.parquet', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
