{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that Python starts in Workflow-folder or else the modules will be screewed up!\n",
    "import sys, os, getpass\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print(module_path)\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "if user == \"sidsel\":\n",
    "    parquet_path = \"/home/sidsel/workspace/sparkdata/parquet\"\n",
    "elif user == \"svanhmic\":\n",
    "    parquet_path = \"/home/svanhmic/workspace/data/DABAI/sparkdata/parquet\"\n",
    "    \n",
    "# Start the logger.\n",
    "import logging\n",
    "logger_tester = logging.getLogger(__name__)\n",
    "logger_tester.setLevel(logging.INFO)\n",
    "logger_file_handler_param = logging.FileHandler('/tmp/workflow_test.log')\n",
    "logger_formatter_param = logging.Formatter('%(asctime)s:%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "logger_tester.addHandler(logger_file_handler_param)\n",
    "logger_file_handler_param.setFormatter(logger_formatter_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.Extension_to_timeit import pretty_time_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(params.output_parameters(parameters))\n",
    "test_params_1 = {'tol': 0.001, 'k': 8, 'maxIter': 10, 'algorithm': 'GaussianMixture', 'seed': 1080866016001745000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sc.defaultMinPartitions)\n",
    "print(sc.defaultParallelism)\n",
    "conf = sc.getConf()\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleaning.ExecuteCleaningWorkflow import ExecuteWorkflow\n",
    "\n",
    "n = [3, 5, 6] # number of samples 1000, 100000, 1000000\n",
    "\n",
    "execution_model = ExecuteWorkflow(dict_params=test_params_1\n",
    "                                ,cols_features=['a','b']\n",
    "                                ,cols_labels=['id','k','dimension'])\n",
    "\n",
    "n_samples = [1000, 10000]#, 1000000]\n",
    "collection_of_data = [parquet_path+'/normal_cluster_n_'+str(i)+'.parquet' for i in n_samples]\n",
    "collection_of_model = []\n",
    "#counts = [i.rdd.getNumPartitions() for i in collection_of_data]\n",
    "#counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data_path in enumerate(collection_of_data):\n",
    "\n",
    "    df_data = spark.read.parquet(data_path)\n",
    "    model_timer = %timeit -o collection_of_model.append(execution_model.execute_pipeline(df_data))  \n",
    "    transformer_timer = %timeit -o execution_model.apply_model(collection_of_model[idx],df_data)\n",
    "    collection_of_model = collection_of_model[:idx+1]\n",
    "    logger_tester.info('Iteration '+str(idx)+' for training model : '+pretty_time_result(model_timer))\n",
    "    logger_tester.info('Iteration '+str(idx)+' for transforming model : '+pretty_time_result(transformer_timer))\n",
    "#merged_df.write.parquet('/home/svanhmic/workspace/data/DABAI/sparkdata/parquet/merged_df_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collection_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### For writing the big ass files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "k = 10\n",
    "samples = [100000000, 1000000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in samples:\n",
    "    means = create_dummy_data.create_means(dim, k, 10)  # [[0, 0, 0], [3, 3, 3], [-3, 3, -3], [5, -5, 5]]\n",
    "    stds = create_dummy_data.create_stds(dim, k)  # [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]\n",
    "    n_samples = create_dummy_data.create_partition_samples(i, k)  # [1000, 10000, 4000, 50]\n",
    "    print(n_samples)\n",
    "    df = create_dummy_data.create_normal_cluster_data_spark(dim, n_samples, means, stds)\n",
    "    #df.show(100)\n",
    "    df.write.parquet('/user/micsas/data/parquet/normal_cluster_n_'+str(i)+'.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
