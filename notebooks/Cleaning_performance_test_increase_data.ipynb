{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svanhmic/workspace/DABAI/Workflows\n"
     ]
    }
   ],
   "source": [
    "# Make sure that Python starts in Workflow-folder or else the modules will be screewed up!\n",
    "import sys, os, getpass\n",
    "from datetime import datetime\n",
    "from py4j import protocol\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print(module_path)\n",
    "\n",
    "user = getpass.getuser()\n",
    "\n",
    "if user == \"sidsel\":\n",
    "    parquet_path = \"/home/sidsel/workspace/sparkdata/parquet\"\n",
    "elif user == \"svanhmic\":\n",
    "    parquet_path = \"/home/svanhmic/workspace/data/DABAI/sparkdata/parquet\"\n",
    "    \n",
    "# Start the logger.\n",
    "import logging\n",
    "logger_tester = logging.getLogger(__name__)\n",
    "logger_tester.setLevel(logging.INFO)\n",
    "logger_file_handler_param = logging.FileHandler('/tmp/'+datetime.now().strftime('workflow_test_%d_%m_%Y.log'))\n",
    "logger_formatter_param = logging.Formatter('%(asctime)s:%(levelname)s:%(name)s:%(message)s')\n",
    "\n",
    "logger_tester.addHandler(logger_file_handler_param)\n",
    "logger_file_handler_param.setFormatter(logger_formatter_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shared.Extension_to_timeit import pretty_time_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(params.output_parameters(parameters))\n",
    "test_params_1 = {'tol': 0.001, 'k': 8, 'maxIter': 10, 'algorithm': 'GaussianMixture', 'seed': 1080866016001745000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.submit.pyFiles',\n",
       "  '/home/svanhmic/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.0-s_2.11.jar,/home/svanhmic/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar,/home/svanhmic/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar,/home/svanhmic/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar,/home/svanhmic/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar'),\n",
       " ('spark.driver.memory', '12g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1504776524676'),\n",
       " ('spark.driver.host', '10.52.1.5'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '39407'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.jars',\n",
       "  'file:/usr/local/share/rand_jars/graphframes-0.5.0-spark2.0-s_2.11.jar,file:/home/svanhmic/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.0-s_2.11.jar,file:/home/svanhmic/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar,file:/home/svanhmic/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar,file:/home/svanhmic/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar,file:/home/svanhmic/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar'),\n",
       " ('spark.files',\n",
       "  'file:/home/svanhmic/.ivy2/jars/graphframes_graphframes-0.5.0-spark2.0-s_2.11.jar,file:/home/svanhmic/.ivy2/jars/com.typesafe.scala-logging_scala-logging-api_2.11-2.1.2.jar,file:/home/svanhmic/.ivy2/jars/com.typesafe.scala-logging_scala-logging-slf4j_2.11-2.1.2.jar,file:/home/svanhmic/.ivy2/jars/org.scala-lang_scala-reflect-2.11.0.jar,file:/home/svanhmic/.ivy2/jars/org.slf4j_slf4j-api-1.7.7.jar')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sc.defaultMinPartitions)\n",
    "print(sc.defaultParallelism)\n",
    "conf = sc.getConf()\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cleaning.ExecuteCleaningWorkflow import ExecuteWorkflow\n",
    "\n",
    "execution_model = ExecuteWorkflow(dict_params=test_params_1\n",
    "                                ,cols_features=['a','b']\n",
    "                                ,cols_labels=['id','k','dimension'])\n",
    "\n",
    "n_samples = [1000,]# 10000, 1000000, 10000000]\n",
    "n_partitions = [80, 200, 400, 600, 800, 1000]\n",
    "collection_of_data = [parquet_path+'/normal_cluster_n_'+str(i)+'.parquet' for i in n_samples]\n",
    "collection_of_model = []\n",
    "#counts = [i.rdd.getNumPartitions() for i in collection_of_data]\n",
    "#counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.21 s ± 61.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "78.7 ms ± 5.16 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "7.43 s ± 328 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "85.7 ms ± 7.98 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "14.9 s ± 239 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "79.6 ms ± 6.54 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "29.5 s ± 5.93 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "98.1 ms ± 6.88 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "32.5 s ± 773 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "87.1 ms ± 4.69 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "46.2 s ± 5.68 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "93.2 ms ± 8.35 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "#Run this bad boy!\n",
    "for jdx, partition in enumerate(n_partitions):\n",
    "    try:\n",
    "        for idx, data_path in enumerate(collection_of_data):\n",
    "\n",
    "            # Strings \n",
    "            str_1 = 'Iteration: {} - Number of partions: {}'\n",
    "            str_2 = 'Iteration: {} - Training model time: {!s}'\n",
    "            str_3 = 'Iteration: {} - Transforming model time {!s}'\n",
    "            \n",
    "            df_data = (spark.\n",
    "                       read.\n",
    "                       parquet(data_path).\n",
    "                       repartition(partition)\n",
    "                       )\n",
    "\n",
    "            iteration = idx+len(collection_of_data)*jdx\n",
    "            logger_tester.info(\n",
    "                str_1.format(iteration, df_data.rdd.getNumPartitions()))\n",
    "            \n",
    "            model_timer = %timeit -r7 -o collection_of_model.append(execution_model.execute_pipeline(df_data))\n",
    "            transformer_timer = %timeit -o execution_model.apply_model(collection_of_model[iteration],df_data)\n",
    "            collection_of_model = collection_of_model[:iteration+1]\n",
    "            \n",
    "            logger_tester.info(\n",
    "                str_2.format(iteration,pretty_time_result(model_timer)))\n",
    "            logger_tester.info(\n",
    "                str_3.format(iteration,pretty_time_result(transformer_timer)))\n",
    "    except protocol.Py4JError as error:\n",
    "        ex_type, ex, tb = sys.exc_info()\n",
    "        logger_tester.warning('Failed with traceback'+ str(error.with_traceback(tb)))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svanhmic/workspace/data/DABAI/sparkdata/parquet/normal_cluster_n_1000.parquet\n",
      "1000\n",
      "/home/svanhmic/workspace/data/DABAI/sparkdata/parquet/normal_cluster_n_10000.parquet\n",
      "10000\n",
      "/home/svanhmic/workspace/data/DABAI/sparkdata/parquet/normal_cluster_n_1000000.parquet\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "for i in collection_of_data:\n",
    "    print(i)\n",
    "    df_data = (spark\n",
    "               .read\n",
    "               .parquet(i)\n",
    "               .repartition(partition)\n",
    "              )\n",
    "    print(df_data.count())\n",
    "#for data in collection_of_data[0]:\n",
    "#    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in samples:\n",
    "    means = create_dummy_data.create_means(dim, k, 10)  # [[0, 0, 0], [3, 3, 3], [-3, 3, -3], [5, -5, 5]]\n",
    "    stds = create_dummy_data.create_stds(dim, k)  # [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]\n",
    "    n_samples = create_dummy_data.create_partition_samples(i, k)  # [1000, 10000, 4000, 50]\n",
    "    print(n_samples)\n",
    "    df = create_dummy_data.create_normal_cluster_data_spark(dim, n_samples, means, stds)\n",
    "    #df.show(100)\n",
    "    df.write.parquet('/user/micsas/data/parquet/normal_cluster_n_'+str(i)+'.parquet', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
